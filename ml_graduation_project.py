# -*- coding: utf-8 -*-
"""ML graduation project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ie7VNnuyRpEfkoG9NWoqIMQBil39sVrr
"""

from google.colab import files

files.upload()

"""## data loading"""

import pandas as pd

df = pd.read_csv( 'train.csv' )
df

df.head( 20 )

"""## data info"""

df.info()

"""##### 33% of location data is missing
##### 0.8% of kewyords is missing

### Tob 15 location
"""

df['location'].value_counts().head( 15 )

"""### Top 15 keyword """

df['keyword'].value_counts().head( 15 )

"""## number of data in each class"""

df['target'].value_counts()

"""## Data cleaning"""

import string
import collections
import numpy as np
import re
import nltk
from nltk.stem import PorterStemmer
from itertools import chain

def cleanText(text):

    text = re.sub(r'#[\S]*','',text)                       #remove hashtage 

    text = re.sub(r'@[\S]*','',text)                       #remove mentions 

    text = re.sub(r'https?:\/\/\S+','',text)                 #remove hyperlink 

    text = re.sub(r'\W+',' ',text)                           #remove emotions

    text = re.sub('\d+','',text)                             #remove digits

    text = re.sub(r'^\s+','',text)                           #remove space in front of text 

    text = re.sub(r'\s+$','',text)                           #remove space in tail text

    return text

df['clean_text'] = df[ 'text' ].apply( cleanText )
df[ ['text', 'clean_text'] ]

"""#### lowering capital letters of text feature"""

df['clean_text'] = df['clean_text'].apply( lambda x: x.lower())
df[ ['text', 'clean_text'] ]

"""#### Tokenization"""

def tokenization( text ):
  
  tokens = re.split( '\W+', text )
  return tokens

df[ 'clean_text' ] = df[ 'clean_text' ].apply( tokenization )
df[ ['text', 'clean_text'] ]

"""####  stop words"""

nltk.download('stopwords')
stopwords = nltk.corpus.stopwords.words('english')

print( stopwords )

"""#### most common stop words"""

def count_stopwords( text ):

    stopWord = [ i for i in text if i in stopwords ]

    return stopWord

common_stopword = df['clean_text'].apply( count_stopwords )

most_common_stopword = collections.Counter( chain( *common_stopword )).most_common( 5 )

most_common_stopword

"""#### removing stop words"""

def remove_stopwords( text ):
    output = [ i for i in text if i not in stopwords ]
    return output

df['clean_text'] = df['clean_text'].apply( remove_stopwords )

df[[ 'text','clean_text' ]]

"""#### stem words"""

from nltk.stem import PorterStemmer

def stem_text( text ):
    
    for word in text:

        ps = PorterStemmer()

        text = [ ps.stem( word ) for word in text ]
        
    return text

df['clean_text'] = df['clean_text'].apply( stem_text )

df[[ 'text', 'clean_text' ]]

"""#### most common words"""

words = df['clean_text']

most_common_words = collections.Counter( chain( *words )).most_common( 3 )

most_common_words

"""### prepare train,val,test sets"""

from sklearn.model_selection import train_test_split

x_train, x_val, y_train, y_val = train_test_split( df['clean_text'], df['target'], test_size = 0.15, random_state = 22 )
print( x_train.shape )
print( x_val.shape )

test_data = pd.read_csv( 'test.csv' )
x_test = test_data[ 'text' ]
x_test.head( 20 )

"""### Tokenization"""

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer

toknizer = Tokenizer( num_words = 20000 )

toknizer.fit_on_texts( x_train )

train_sequnces = toknizer.texts_to_sequences( x_train )

print(train_sequnces)

"""### pad sequence"""

from tensorflow.keras.preprocessing.sequence import pad_sequences

train_padded_sequence = pad_sequences( train_sequnces,truncating ='post', maxlen = 20 )

train_padded_sequence

val_sequnces = toknizer.texts_to_sequences( x_val )
test_sequnces = toknizer.texts_to_sequences( x_test )

print( val_sequnces )
print( test_sequnces )

val_padded_sequence = pad_sequences( val_sequnces, truncating = 'post', maxlen = 20 )

test_padded_sequence = pad_sequences( test_sequnces, truncating = 'post', maxlen = 20 )

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense ,LSTM,Embedding ,GlobalAveragePooling1D,Dropout
from tensorflow.keras.callbacks import ModelCheckpoint


model = Sequential()

model.add( Embedding( 20000 , 64, input_length = 20 ))

model.add( LSTM( 32, return_sequences = True ))
model.add( Dropout( 0.4 ))

model.add( LSTM( 16, return_sequences = True ))
model.add( Dropout( 0.3 ))

model.add( LSTM( 8 ))
model.add( Dropout( 0.4 ))

model.add( Dense( 1, activation = 'sigmoid' ))

model.summary()

model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint('checkpoint.h5', monitor = 'val_accuracy', save_best_only = True)

model.compile( 'adam', 'binary_crossentropy', metrics = ['accuracy'] )

history = model.fit( train_padded_sequence, y_train, batch_size = 128, epochs = 10, validation_data = ( val_padded_sequence, y_val ), callbacks = [model_checkpoint_callback] )

"""### chart of accuracy and loss"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

plt.plot( history.history['accuracy'], 'b' )
plt.plot( history.history['loss'], 'r' )

"""### making predictions"""

predictions = model.predict( test_padded_sequence )
predictions = predictions.round()
predictions[ :20 ]

"""### saving the model"""

model.save('model.h5')

